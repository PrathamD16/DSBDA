{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4c064b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they wandered into a strange tiki bar on the edge of the small beach town every manager should be able to recite at least ten nursery rhymes backward find bar near beach\n",
      "iguanas were falling out of the trees the sunblock was handed to the girl before practice but the burned skin was proof she did not apply it better she must have used sunblock\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "text1 = \"They wandered into a strange Tiki bar on the edge of the small beach town. Every manager should be able to recite at least ten nursery rhymes backward. Find bar near beach\"\n",
    "text2 = \"Iguanas were falling out of the trees. The sunblock was handed to the girl before practice, but the burned skin was proof she did not apply it. Better she must have used sunblock\"\n",
    "text1 = text1.lower()\n",
    "text2 = text2.lower()\n",
    "text1 = text1.translate(str.maketrans('','',string.punctuation))\n",
    "text2 = text2.translate(str.maketrans('','',string.punctuation))\n",
    "print(text1)\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f49f5fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['they wandered into a strange tiki bar on the edge of the small beach town every manager should be able to recite at least ten nursery rhymes backward find bar near beach']\n",
      "['iguanas were falling out of the trees the sunblock was handed to the girl before practice but the burned skin was proof she did not apply it better she must have used sunblock']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "tokenized_text1 = nltk.sent_tokenize(text1)\n",
    "tokenized_text2 = nltk.sent_tokenize(text2)\n",
    "print(tokenized_text1)\n",
    "print(tokenized_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b214d3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'should', \"you've\", \"she's\", 'on', 'being', 'who', 'how', 've', 'weren', 'do', 'now', 'this', 'both', \"hasn't\", 'under', 'before', 'himself', 'few', 'hers', 'm', 'don', 'just', 'she', 'having', 'his', 'for', 'ma', 'about', 'my', 'ain', 'these', 'does', 'you', 'up', 'when', \"mustn't\", 'so', 'd', \"needn't\", 'shouldn', 'theirs', 'the', 'hasn', 'is', 'itself', 'couldn', \"hadn't\", 'only', 'with', 'over', \"don't\", 'll', \"isn't\", 'mightn', 'nor', 'ours', 'above', 'it', 'between', 'didn', 'was', 'those', 'our', 's', 'had', 'each', 'at', 'can', \"wasn't\", 'during', \"you'd\", \"you're\", 'here', \"aren't\", 'yours', 'of', \"won't\", 'any', 'no', 'such', \"couldn't\", 'their', 'or', 'and', 'against', 'hadn', 'ourselves', \"mightn't\", 'been', 'her', 'we', 'they', 'by', 'herself', 'there', 're', \"didn't\", 'isn', \"that'll\", 'but', 'that', 'to', 'needn', 'down', \"haven't\", 'below', \"doesn't\", 'which', \"should've\", 'won', 'again', 'into', 'yourselves', 'because', 'then', 'are', 'all', 'wouldn', 'if', 'did', 'has', 'same', 'some', 'whom', 'he', 'what', 'its', 'own', 'doing', 'aren', 'yourself', 'am', 'o', 'off', 'why', \"wouldn't\", 'more', 'shan', 'have', 'them', 'after', 'where', 'in', 'other', 'not', 'doesn', 'too', 'your', 'from', 'me', 'a', 'until', 'once', 'will', 'while', 'y', 'very', 'wasn', 'myself', \"it's\", 'further', \"shan't\", 'most', \"you'll\", 'than', 'through', 't', 'i', 'an', 'haven', \"shouldn't\", 'him', 'be', 'as', 'were', \"weren't\", 'themselves', 'out', 'mustn'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6fab7e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Stop Words Removal Text 1*****\n",
      "\n",
      "['wandered', 'strange', 'tiki', 'bar', 'edge', 'small', 'beach', 'town', 'every', 'manager', 'able', 'recite', 'least', 'ten', 'nursery', 'rhymes', 'backward', 'find', 'bar', 'near', 'beach']\n",
      "\n",
      "\n",
      "****Stop Words Removal Text 2*****\n",
      "\n",
      "['iguanas', 'falling', 'trees', 'sunblock', 'handed', 'girl', 'practice', 'burned', 'skin', 'proof', 'apply', 'better', 'must', 'used', 'sunblock']\n"
     ]
    }
   ],
   "source": [
    "#Stop words removal\n",
    "filtered1 = []\n",
    "filtered2 = []\n",
    "for i in tokenized_text1:\n",
    "    wordlist = nltk.word_tokenize(i)\n",
    "    for j in wordlist:\n",
    "        if not j in stopWords:\n",
    "            filtered1.append(j)\n",
    "for i in tokenized_text2:\n",
    "    wordlist = nltk.word_tokenize(i)\n",
    "    for j in wordlist:\n",
    "        if not j in stopWords:\n",
    "            filtered2.append(j)\n",
    "\n",
    "print(\"****Stop Words Removal Text 1*****\\n\")\n",
    "print(filtered1)\n",
    "print(\"\\n\")\n",
    "print(\"****Stop Words Removal Text 2*****\\n\")\n",
    "print(filtered2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f19a60db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Stemmed text 1*****\n",
      "\n",
      "{'wandered': 'wander', 'strange': 'strang', 'tiki': 'tiki', 'bar': 'bar', 'edge': 'edg', 'small': 'small', 'beach': 'beach', 'town': 'town', 'every': 'everi', 'manager': 'manag', 'able': 'abl', 'recite': 'recit', 'least': 'least', 'ten': 'ten', 'nursery': 'nurseri', 'rhymes': 'rhyme', 'backward': 'backward', 'find': 'find', 'near': 'near'}\n",
      "\n",
      "\n",
      "*****Stemmed text 2*****\n",
      "\n",
      "{'iguanas': 'iguana', 'falling': 'fall', 'trees': 'tree', 'sunblock': 'sunblock', 'handed': 'hand', 'girl': 'girl', 'practice': 'practic', 'burned': 'burn', 'skin': 'skin', 'proof': 'proof', 'apply': 'appli', 'better': 'better', 'must': 'must', 'used': 'use'}\n"
     ]
    }
   ],
   "source": [
    "#stemming\n",
    "stemmed1 = {}\n",
    "stemmed2 = {}\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "for i in filtered1:\n",
    "    stemmed1.update({i : ps.stem(i)})\n",
    "for i in filtered2:\n",
    "    stemmed2.update({i : ps.stem(i)})\n",
    "print(\"*****Stemmed text 1*****\\n\")\n",
    "print(stemmed1)\n",
    "print(\"\\n\")\n",
    "print(\"*****Stemmed text 2*****\\n\")\n",
    "print(stemmed2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "374920b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******Lemmetized Text 1*****\n",
      "\n",
      "{'wandered': 'wandered', 'strange': 'strange', 'tiki': 'tiki', 'bar': 'bar', 'edge': 'edge', 'small': 'small', 'beach': 'beach', 'town': 'town', 'every': 'every', 'manager': 'manager', 'able': 'able', 'recite': 'recite', 'least': 'least', 'ten': 'ten', 'nursery': 'nursery', 'rhymes': 'rhyme', 'backward': 'backward', 'find': 'find', 'near': 'near'}\n",
      "\n",
      "\n",
      "******Lemmetized Text 2*****\n",
      "\n",
      "{'iguanas': 'iguanas', 'falling': 'falling', 'trees': 'trees', 'sunblock': 'sunblock', 'handed': 'handed', 'girl': 'girl', 'practice': 'practice', 'burned': 'burned', 'skin': 'skin', 'proof': 'proof', 'apply': 'apply', 'better': 'good', 'must': 'must', 'used': 'used'}\n"
     ]
    }
   ],
   "source": [
    "#lemmitization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmed1 = {}\n",
    "lemmed2 = {}\n",
    "for i in filtered1:\n",
    "    lemmed1.update({i:lemmatizer.lemmatize(i)})\n",
    "    \n",
    "for i in filtered2:\n",
    "    lemmed2.update({i:lemmatizer.lemmatize(i, pos=\"a\")})\n",
    "    \n",
    "print(\"******Lemmetized Text 1*****\\n\")\n",
    "print(lemmed1)\n",
    "print(\"\\n\")\n",
    "print(\"******Lemmetized Text 2*****\\n\")\n",
    "print(lemmed2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5322940c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****POS OF TEXT 1\n",
      "\n",
      "[('wandered', 'JJ'), ('strange', 'JJ'), ('tiki', 'NN'), ('bar', 'NN'), ('edge', 'VBP'), ('small', 'JJ'), ('beach', 'NN'), ('town', 'NN'), ('every', 'DT'), ('manager', 'NN'), ('able', 'JJ'), ('recite', 'NN'), ('least', 'JJS'), ('ten', 'JJ'), ('nursery', 'JJ'), ('rhymes', 'NNS'), ('backward', 'RB'), ('find', 'VBP'), ('bar', 'NN'), ('near', 'IN'), ('beach', 'NN')]\n",
      "****POS OF TEXT 2\n",
      "\n",
      "[('iguanas', 'NNS'), ('falling', 'VBG'), ('trees', 'NNS'), ('sunblock', 'RB'), ('handed', 'VBD'), ('girl', 'JJ'), ('practice', 'NN'), ('burned', 'VBD'), ('skin', 'JJ'), ('proof', 'NN'), ('apply', 'VB'), ('better', 'JJR'), ('must', 'MD'), ('used', 'VBN'), ('sunblock', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#Parts Of Speech\n",
    "pos1 = []\n",
    "pos2 = []\n",
    "print(\"****POS OF TEXT 1\\n\")\n",
    "print(nltk.pos_tag(filtered1))\n",
    "print(\"****POS OF TEXT 2\\n\")\n",
    "print(nltk.pos_tag(filtered2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c8af67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
